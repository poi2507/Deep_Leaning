{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 순환 신경망(RNN)\n",
    "\n",
    "피드포워드(Feed Forward) : 흐름이 단방향인 신경망 피드포워드의 단점은 시계열 데이터를 잘 다루지 못한다.\n",
    "시계열 데이터의 성질을 충분히 학습할 수 없다. $\\rightarrow$ **순환신경망이 등장한 배경**\n",
    "\n",
    "\n",
    "### 언어모델\n",
    "언어 모델은 단어 나열에 확률을 부여한다. 특정한 단어 시퀀스에 대해서 그 시퀀스가 일어날 가능성이 어느정도인지 확률로 평가한다.\n",
    "\n",
    "단어가 $W_1, W_2, \\cdots, W_m$ 이라는 순서대로 출연한 확률을 $P(W_1, \\cdots, W_m)$으로 나타낸다. 이 확률을 여러 사건이 동시에 일어날 확률 이므로 동시 확률이라고 부른다.\n",
    "\n",
    "![](https://blog.kakaocdn.net/dn/dOYvMK/btqTSTuz1su/f1DzIk9u2nWUE3tY1eLVk1/img.png)\n",
    "\n",
    "곱셈 정리를 사용하여 m개의 단어의 동시확률 $P(W_1,\\cdots, W_m)$을 사후 확률로 나타낼수 있다.\n",
    "\n",
    "머신러닝이나 통계학에서 마르코프 연쇄 또는 마르코프 모델이라는 말을 자주 듣는다. 마르코프 연쇄란 미래의 상태가 현재 상태에만 의존해서 발생하는 것을 말한다.\n",
    "\n",
    "\n",
    "\n",
    "CBow 모델의 은닉층에서는 단어 벡터들이 더해지므로 맥락이 무시된다.\n",
    "\n",
    "|CBow|신경 확률적 모델|\n",
    "|---|---|\n",
    "|![](./img/IMG_24B8A9F7C428-1.jpeg)|![](./img/IMG_B1C83D272BDB-1.jpeg)|\n",
    "\n",
    "RNN은 맥락이 아무리 길더라도 그 맥의 정보를 기억하는 메커니즘을 갖추고 있다. word2vec은 단어의 분산표현을 얻을 목적으로 고안된 기법이다. 따라서 이를 언어 모델로 사용하진 않는다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### RNN(Recurrent Neural Network)\n",
    "\n",
    "\n",
    "\n",
    "순환하기 위해서는 닫힌 경로가 필요하다. 데이터가 정보를 끊임없이 갱신하다.\n",
    "\n",
    "<img src = \"https://blog.kakaocdn.net/dn/w9Wfk/btqTvRJdz8m/GL3aDzhlGB5ZKX0hGUMt30/img.png\">\n",
    "\n",
    "$x_t$(단어의 분산표현)를 입력받는다, t는 시각을 뜻한다.\n",
    "\n",
    "<img src = './img/IMG_71313BEE4FDE-1.jpeg'>\n",
    "\n",
    "\n",
    "다수의 RNN 계층 모두가 실제로는 '같은 계층'인 것이 지금까지의 신경망과는 다르다.\n",
    "\n",
    "각 시각의 RNN 계층은 그 계층으로의 입력과 1개의 RNN 계츨으로의 출력을 받는다. 두 정보를 바탕으로 현시각의 출력을 계산한다.\n",
    "\n",
    "$$h_t = tanh(h_{t-1} w_h + x_t w_x + b)$$\n",
    "\n",
    "RNN에는 가중치가 2개 있다. 하나는 입력 $x_t$를 h로 변환하기 위한 $w_x$, RNN 출력을 다음 시각의 출력으로 변환하기위한 기증치 $w_h$ 또한 편행 b\n",
    "$h_t$는 다른 계층을 향해 출력되는 동시에 다음 시각의 RNN 계층을 향해 오른쪽으로 출련된다.\n",
    "\n",
    "\n",
    "\n",
    "### BPTT\n",
    "\n",
    "\n",
    "RNN 계층은 가로로 펼쳐진 신경망으로 볼 수 있다.\n",
    "\n",
    "BPTT(BackPropagation Through Time) : 시간 방향으로 펼친 오차역전파법\n",
    "\n",
    "시계열 데이터의 시간 크기가 커지는 것에 비례해서 BPTT가 소비하는 컴퓨팅 자원도 증가한다. 시간 크기가 커지면 역전파시의 기울기가 불안정해진다.\n",
    "\n",
    "BPTT로 기울기를 구하려면, 매 시각 RNN 계츨의 중간 메모리를 유지해두지 않으면 안된다.\n",
    "\n",
    "큰 시계열 데이터를 처리할 때, 신경망 연결을 적당한 길이로 끊는다.\n",
    "\n",
    "Truncated BPTT: 너무 길어진 신경망을 적당한 길이로 자르고 잘라낸 신경망을 오차역전파법을 수행한다.\n",
    "\n",
    "역전파의 연결만 끊는다 순전파의 연결을 유지한다.\n",
    "RNN에서 Truncated BPTT를 수행할떄는 데이터를 순서대로 입력해야한다.\n",
    "\n",
    "<img src = \"./img/IMG_19D478395C38-1.jpeg\">\n",
    "\n",
    "\n",
    "Time RNN 계층은 T개의 RNN 계층으로 구성한다.\n",
    "![](./img/IMG_5316F7DBE529-1.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### RNNLM(RNN Language Model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](./img/IMG_02D8985287CE-1.jpeg)\n",
    "\n",
    "첫번째 층을 Embedding 계층이다. 이 계층은 단어 ID를 단어의 분산 표현으로 변환한다. 그 분산 표현은 RNN 계층으로 입력된다.\n",
    "RNN 계층은 은닉 상태를 다음 층으로 출력함과 동시에 다음 시각의 RNN 계층\n",
    "으로 출력한다. 그리고 RNN 계층이 위로 출력한 은닉상태는 Affine 계층을 거쳐 softmax 계층으로 이어진다.\n",
    "\n",
    "RNNLM은 지금까지 입력된 다어를 '기억'하고 그것을 바탕으로 다음에 출현할 단어를 예측한다.\n",
    "RNN 게층이 과거에서 현재로 데이터를 계속 흘려주며서 과거의 정보를 인코딩해 저장할 수 있는 것이다.\n",
    "\n",
    "\n",
    "\n",
    "### 언어 모델의 평가\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "언어 모델은 주어진 과거 단어로 부터 다음에 출현할 단어의 확률 분포를 출력한다.\n",
    "언어 모델의 성능을 평가하는 척도로 퍼플렉시티(perplexity)를 자주 이용한다.\n",
    "**퍼플렉시티는 확률의 역수이다.**\n",
    "\n",
    "![](./img/IMG_1D1373B31FA3-1.jpeg)\n",
    "\n",
    "perplexity는 작을수록 좋다.\n",
    "\n",
    "- 이 값은 분기수로 이해할 수 있다. 분기수란 다음에 취할 수 있는 선택사항 수. 분기수가 1.25라는 것은 다음에 후보로 출연할 단어가 1개 정도로 좁혀졌다는 의미이다.\n",
    "\n",
    "- 입력데이터가 여러개 일떄의 perplexity $\\rightarrow L=-\\frac{1}{N} \\sum_n \\sum_k t_{nk} log(y_{nk})$\n",
    "- $perplexity = e^L$\n",
    "\n",
    "\n",
    "## 게이트가 추가된 RNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "베이직한 RNN은 구조가 단순하여 구현은 쉽지만 성능이 좋지 못하다. 그 원인은 시계열 데이터에서 시간적으로 멀리 떨어진, 장기 Long term 의존 관계를 잘 학습할 수 없다.\n",
    "\n",
    "LSTM이나 GRU에는 '게이트'라는 구조가 더해져 있는데. 이 게이트 덕분에 시계열 데이터의 장기 의존 관계를 학습할 수 있다.\n",
    "\n",
    "베이직한 RNN의 문제점은 BPTT에서 기울기 소실 혹은 기울기 폭발이 일어나기 때문이다.\n",
    "- 시간 방향의 기울기에 주목하면 역전파로 전해지는 기울기는 차례로 'tanh', '+', 'matmul' 연산을 통과하게 된다. '+'의 역전파는 상류에서 전해지는 기울기를 하류로 흘려보낼 뿐이다.\n",
    "\n",
    "![](https://www.tutorialexample.com/wp-content/uploads/2020/08/the-graph-of-tanhx-function-derivative.png)\n",
    "\n",
    "$\\frac{dy}{dx}$의 값이 0~1 사이 즉 역전파에서 기울기가 tanh 노드를 지날떄 마나 값이 작아진다.\n",
    "\n",
    "Matmul 노드에서의 역전파는 $dhw_r^t$라는 행렬곱으로 기울기를 계산한다. 그리고 같은 시계열의 데이터 크기 만큼 반복한다. 여기서 주목할 점은 매번 똑같은 가중치인 $w_r$가 사용된다는 것이다.\n",
    "\n",
    "$w_r$가 1보다 크면 기울기가 지수적으로 증가하고, 1보다 작으면 지수적으로 감소한다.\n",
    "\n",
    "기울기 폭발의 대책으로는 전통적인 기법이 있다.\n",
    "\n",
    "**기울기 클리핑(gradient clipping)**\n",
    "\n",
    "if $$\\lVert \\hat g \\rVert \\geq threshold: \\hat g = \\frac{threshold}{\\lVert \\hat g \\rVert}\\hat g$$\n",
    "\n",
    "\n",
    "#### LSTM의 인터페이스\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1024/0*243z3lyg05TswC9D)\n",
    "\n",
    "\n",
    "\n",
    "LSTM 계층에는 c라는 경로가 있다. c를 기억셀이라 하며 LSTM의 기억 매커니즘이다.\n",
    "\n",
    "기억셀의 특징은 데이터를 자기자신하고만(LSTM 게층 내에서만) 주고 받는다는 것이다. 즉, LSTM 계층내에서만 완결되고, 다른 계층으로는 출력하지 않는다.\n",
    "\n",
    "LSTM의 출력은 은닉벡터 h 뿐이다. 기억셀 c는 외부에서는 보이지 않는다.\n",
    "\n",
    "$c_t$에는 과거로부터  시작 t까지에 필요한 모든 정보가 저장되어있다고 가정한다.\n",
    "기억셀 $c_t$는 3개의 입력 $(c_{t-1}, h_{t-1}, x_t)$으로 부터 '어떤 계산'을 수행하여 구할 수 있다.\n",
    "\n",
    "$h_t = tanh(c_t)$이다. 이는 $c_t$의 각 요소에 tanh 함수를 적용한다는 뜻이다.\n",
    "\n",
    "기억셀 $c_t$와 은닉상태 $h_t$의 원소수는 같다.\n",
    "\n",
    "게이트는 데이터의 흐름을 제어한다.\n",
    "LSTM에서 사용하는 게이트는 '열기/닫기' 뿐만 아니라 어느정도 열지를 조절할 수 있다. 어느정도를 열림상태 openness라고 부른다.\n",
    "\n",
    "\n",
    "\n",
    "![](./img/IMG_49CEF7B36468-1.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "게이틀의 열림 상태는 0.0 ~ 0.1 사이의 실수로 나타낸다. 게이트는 게이트의 연림 상태를 제어하기 위해서 전용 가중치 매개변수를 이용하며, 이 가중치는 학습데이터로부터 갱신된다.\n",
    "열림 상태를 구할 때는 시그모이드 함수를 사용한다.\n",
    "\n",
    "#### output 게이트\n",
    "\n",
    "\n",
    "\n",
    "이 게이트는 다음 은닉 상태 $h_t$의 출력을 담당하는 게이트이므로 output 게이트라고한다.\n",
    "\n",
    "$$ O = \\sigma(x_t W^{(o)}_x + h_{t-1} W^{(o)}_h + b^{(o)})$$\n",
    "\n",
    "\n",
    "\n",
    "![](./img/IMG_E358232B139E-1.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "$\\sigma$의 출력을 O라고 하면 $h_t$ O와 $tanh(c_t)$의 곱으로 계산된다. 여기서 말하는 곱은 원소별 곱이며 이것을 아다마르 곱이라고도 한다.\n",
    "\n",
    "$$h_t = O \\odot tanh(c_t)$$\n",
    "\n",
    "\n",
    "\n",
    "#### forget 게이트\n",
    "\n",
    "\n",
    "\n",
    "$c_{t-1}$의 기억중에 불필요한 기억을 잊게 해주는 게이트\n",
    "\n",
    "![](./img/IMG_E16A2B42649C-1.jpeg)\n",
    "\n",
    "$$f = \\sigma(x_t W_x^{(f)} + h_{t-1}W^f_h + b^f)$$\n",
    "\n",
    "$$c_t = f \\odot c_{t-1}$$\n",
    "\n",
    "![](./img/IMG_A0CF4CA5383A-1.jpeg)\n",
    "\n",
    "\n",
    "#### tanh 노드\n",
    "\n",
    "\n",
    "tanh 노드가 계산한 결과가 이전 시각의 기억셀 $c_{t-1}$에 더해진다. 기억셀에 새로운 정보가 추가된 것이다. 이 tanh 노드는 게이트가 아니며 새로운 정보를 기억셀에\n",
    "추가하는 것이 목적이다. 활성화 함수로 sigmoid가 아닌 tanh 함수를 사용한다.\n",
    "\n",
    "$$g = tanh(x_tW^{(g)}_x + h_{t-1}W_h^{(g)} + b^{(g)})$$\n",
    "\n",
    "\n",
    "#### input 게이트\n",
    "\n",
    "\n",
    "\n",
    "![](./img/IMG_F2889C479F30-1.jpeg)\n",
    "\n",
    "\n",
    "input gate는 g의 각 원소가 서로 추가되는 정보로써 가치가 얼마나 큰지 판단한다. 다른 과넞ㅁ에서 보면 input gate에 의해 가중된 정보를 추가하는 것이다.\n",
    "\n",
    "$$i = \\sigma(x_t W_x^{(i)} + h_{t-1}W^{(i)}_h + b^{(i)})$$\n",
    "\n",
    "\n",
    "\n",
    "### LSTM의 기울기 흐름\n",
    "\n",
    "\n",
    "\n",
    "기억셀의 역전파에서는 '+', 'x' 노드만 지나게 된다. '+'노드는 상류의 흐름을 그대로 하류에 넘겨준다. 또한 $'\\times'$노드는 행렬곱이 아닌 원소별곱(아다마르 곱)\n",
    "을 계산한다. **매 시각 다른 게이트 값을 이용해 원소별 곱을 계산한다.** 새로운 게이트 값을 이용하므로 곱셈의 효과가 누적되지 않아 기울기 손실이 발생하기 어렵게 된다.\n",
    "\n",
    "$'\\times'$ 노드의 계산은 forget 게이트가 제어한다. forget gate가 '잊어야 한다' 고 판단한 기억셀의 원소는 기울기가 작아지고 반대의 경우에는 기울기가 약화되지 않은 채로 과거로 전해진다.\n",
    "\n",
    "\n",
    "### LSTM 계층의 다각화\n",
    "\n",
    "\n",
    "\n",
    "![](./img/IMG_5594A564917C-1.jpeg)\n",
    "\n",
    "LSTM 계층을 2층, 3층 식으로 어려겹 쌓으면 언어 모델의 정확도가 향상되리라 기대할 수 있다.\n",
    "\n",
    "\n",
    "### 과적합 억제\n",
    "\n",
    "\n",
    "\n",
    "#### 드롭아웃에 의한 과적합 억제\n",
    "\n",
    "\n",
    "층을 깊게 쌓음으로써 표현력이 풍부한 모델을 만들 수 있다. 그러나 이런 모델은 종종 overfitting을 일으킨다.\n",
    "\n",
    "과적합을 억제하기 위한 전통적인 방법으로 훈련 데이터양 늘리기, 모델의 복잡도 줄이기 그 외에는 모델의 복잡도에 패널티를 주는 정규화가 효과적이다.\n",
    "\n",
    "드롭아웃은 무작위로 뉴런을 선택하여 선택한 뉴런을 무시한다 RNN에서 시계열 방향으롣 드롭아웃을 넣어버리면 시간이 흐름에 따라 정보가 사라질 수 있다. 즉, 흐르는 시간에 비례해서 드롭아웃에 의한 노이즈가 축적된다.\n",
    "\n",
    "\n",
    "![](./img/IMG_4C5476E6FDC2-1.jpeg)\n",
    "\n",
    "\n",
    "이렇게 구성하면 시간 방향으로 아무리 진행해도 정보를 잃지 않는다.\n",
    "\n",
    "변형 드롭아웃을 통해서 시간방향으로 적용하는데 성공했다. 변형 드롭아웃은 깊이 방향은 물론이고 시간뱡향으로도 이용할 수 있다. 같은 계층에 속한 드롭아웃들은 같은 마스크(mask)\n",
    "를 공유한다. 마스크란 통과/차단을 결정하는 이진 형태의 무작위 패천이다.\n",
    "\n",
    "![](./img/IMG_76BFA8F0A310-1.jpeg)\n",
    "\n",
    "같은 계층의 드롭아웃끼리 마스크를 공유함으로써 마스크가 '고정'된다. 그 결과 정보를 읽게 되는 방법도 '고정' 되므로 일번적인 드롭아웃과 달리 지수적 손실되는 사태를 피할 수 있다.\n",
    "\n",
    "\n",
    "#### 가중치 공유\n",
    "\n",
    "\n",
    "언어 모델을 계선하는 간단한 트릭중에서 가중치 공유가 있다.\n",
    "\n",
    "![](./img/IMG_86DC3A83F5A2-1.jpeg)\n",
    "\n",
    "두 계층의 가중치를 공유함으로써 학습하는 매개변수 수가 크게 줄어드는 동시에 정확도가 향상되는 일석이조의 기술이다.\n",
    "\n",
    "\n",
    "## RNN을 사용한 문장 생성\n",
    "\n",
    "seq2seq란 시계열에서 시계열로를 뜻하는 말로 한 시계열 데이터를 다른 시계열 데이터로 변환하는 것을 말한다.\n",
    "\n",
    "언어 모델은 지금까지 주어진 단어들에서 다음에 출현할 단어의 확률 분포를 출력한다.\n",
    "이 결과를 기초로 다음 단어를 생성할려면 어떻게 해야할까\n",
    "첫번쨰로 확률이 가장 높은 단어를 선택하는 방법이 있다.\n",
    "확률이 가장 높은 단어를 선택할 뿐이므로 결과가 일정하게 정해지는 '결정적인' 방법이다. 또한, '확률적' 으로 선택하는 방법도 있다.\n",
    "확률이 높은 단어가 선택되기 쉽고, 확률이 낮은 단어는 선택되기 어려워진다. 이 방식에서는 선택되는 단어가 매번 다를수 있다.\n",
    "\n",
    "생성한 문장은 훈련데이터에는 존재하지 않는, 말 그대로 서로 생성된 문장이다. 왜냐하면 언어모델은 훈련데이터를 암기하는 것이 아니라 훈련 데이터에서 사용된 단어의 정렬 패턴을 학습한 것이기 때문이다.\n",
    "\n",
    "### seq2seq\n",
    "\n",
    "seq2seq를 Encoder - Decoder 모델이라고도 한다.\n",
    "Encoder는 입력데이터를 인코딩하고 Decoder는 인코딩된 데이터를 디코딩한다.\n",
    "\n",
    "![](https://www.oreilly.com/library/view/hands-on-natural-language/9781789139495/assets/edcaff4c-fedc-4a9f-92fd-7d4b2d6540fb.png)\n",
    "\n",
    "\n",
    "Encoder가 인코딩한 정보에는 번역에 필요한 정보가 조밀하게 응집되어 있다. Decoder는 조밀하게 응축된 이 정보를 바탕으로 도착어 문장을 생성한다.\n",
    "![](./img/IMG_0734F3349B45-1.jpeg)\n",
    "\n",
    "Encoder는 RNN을 이용해 시계열 데이터를 h라는 은닉 상태 벡터로 변환한다.\n",
    "\n",
    "Encoder가 출력하는 벡터 h는 LSTM 계층이 마지막 은닉 상태이다. 이 마자막 은닉 상태 h에 입력문장을 번역하는 데 필요한 정보가 인코딩된다.\n",
    "**여기서 중요한 점은 LSTM의 은닉상태 h는 고정길이 벡터라는** 사실이다.\n",
    "결국 인코딩한다는 것은 임의 길이의 문장을 고정길이 벡터롤 변환하는 작업이 된다.\n",
    "\n",
    "![](./img/IMG_3C7B2935FE75-1.jpeg)\n",
    "\n",
    "\n",
    "Decoder는 앞절의 신경망과 완전히 다른 구성이다 단 한기지만 빼고 말이다. 바로 LSTM 계층이 벡터 h를 입력으로 받는 다는 것이다.\n",
    "\n",
    "![](./img/IMG_C68CDBE0FED7-1.jpeg)\n",
    "\n",
    "LSTM 계층의 은닉상태가 Encoder와 Decoder를 이어주는 '가교' 역할을 한다.\n",
    "\n",
    "\n",
    "\n",
    "### 가변길이의 시계열데이터\n",
    "\n",
    "\n",
    "가변길이의 시계열 데이터를 미니배치로 학습하기 위한 가장 단순한 방법은 패딩을 사용하는 것이다.\n",
    "패딩이란 원래의 데이터에 의미없는 데이터를 채워넣어 데이터의 길이를 균일하게 맞추는 것이다.\n",
    "\n",
    "![](./img/IMG_E3A5FDBA441B-1.jpeg)\n",
    "\n",
    "\n",
    "패딩을 적용해 가변길이의 시계열데이터도 처리할 수 있다 그러나 원래 존재하지 않던 패딩을 문자까지 seq2seq가 처리하게 된다.\n",
    "\n",
    "Decoder에 입력된 데이터가 패딩이라면 손실의 결과를 반영하지 않도록 한다. (Softmax with Loss 계층에 '마스크' 기능을 추가해 해결할 수 있다.)\n",
    "\n",
    "### seq2seq 개선\n",
    "\n",
    "seq2seq를 세분화하여 학습 '속도'를 개선하고자 한다.\n",
    "\n",
    "첫번째는 쉬운 트릭으러 입력데이터 반전이다.\n",
    "\n",
    "![](./img/IMG_CF7258A81702-1.jpeg)\n",
    "\n",
    "이 트릭을 사용하면 많으 경우 학습 진행이 빨라져서 결과적으로 최종 정확도가 좋아진다.\n",
    "직관적으로 기울기의 전파가 원활해지기 때문이라고 생각된다.\n",
    "\n",
    "### 엿보기(peeky)\n",
    "\n",
    "\n",
    "![](./img/IMG_B2251154C0E9-1.jpeg)\n",
    "\n",
    "\n",
    "모든 시각의 Affine 계층과 LSTM 계층에 Encoder의 출력 h를 전해준다. h의 정보를 여러 계층에서 공유함을 알수 있다.\n",
    "\n",
    "\n",
    "\n",
    "## 어텐션\n",
    "\n",
    "seq2seq를 좀더 강력하게 하는 어텐션 매커니즘\n",
    "\n",
    "어텐션 매커니즘 덕분에 seq2seq는 필요한 정보에만 주목 할 수 있게된다.\n",
    "\n",
    "\n",
    "### seq2seq의 문제점\n",
    "\n",
    "\n",
    "seq2seq에서는 Encoder가 시계열 데이터를 인코딩한다.\n",
    "그리고 인코딩된 정보를 Decoder로 전달한다. 이때 Encoder의 출ㄹ력은 '고정길이 벡터'\n",
    "였다. '고정 길이' 리는데 큰 문제가 잠재되어있다.\n",
    "\n",
    "고정 길이 벡터라 함은 입력 문장의 길이에 관계없이 항상 같은 길이의 벡터로 변환한다는 뜻이다.\n",
    "\n",
    "### Encoder의 개선\n",
    "\n",
    "Encoder 출력의 길이는 입력문장의 길이에 따라 바뀌는 것이 좋다. 이점이 Encoder의 개선 포인트다.\n",
    "\n",
    "![](./img/IMG_5FB17EB7B0A8-1.jpeg)\n",
    "\n",
    "\n",
    "각 시각의 은닉상태 벡터를 모두 이용하면 입력된 단어와 같은 수의 벡터를 얻을 수 있다.\n",
    "\n",
    "주목할 것은 LSTM 계층의 은닉 상태의 '내용'이다. 시각별 LSTM 계층의 은닉상태에서 직전에 입력된 단어에 대한 정보가 많이 포함되어 있다는 사실이다.\n",
    "\n",
    "Encoder의 은닉상태를 모든 시각만큼 꺼냈을 뿐이지만 이 작은 개선 덕분에 Encoder는 입력 문자의 길이에 비례한 정보를 인코딩할 수 있게 되었다.\n",
    "\n",
    "### Decoder 개선\n",
    "\n",
    "\n",
    "$h_s$가 Decoder에 전달되어 시계열 변환이 이뤄진다.\n",
    "\n",
    "\n",
    "![](./img/IMG_2ECA5E6B74CE-1.jpeg)\n",
    "\n",
    "\n",
    "단순한 seq2seq에서는 Encoder의 마지막 은닉 상태 벡터만을 Decoder에 넘겼다. 더 정확하게 말하면 Encoder의 LSTM 계층의 '마지막' 은닉 상태를 Decoder의 LSTM 계층의 '첫'은닉 상태로 설정한 것이다.\n",
    "\n",
    "\n",
    "### 어텐션\n",
    "\n",
    "\n",
    "필요한 정보에만 주목하여 그정보로 부터 시계열 변환을 수행하는것 이 구조를 어텐션이라 한다.\n",
    "\n",
    "\n",
    "![](./img/IMG_B9D51EC9E113-1.jpeg)\n",
    "\n",
    "\n",
    "![](./img/IMG_EE5304E7E77D-1.jpeg)\n",
    "\n",
    "\n",
    "각 단어의 중요도를 나타내는 '가중치'를 구한다.\n",
    "\n",
    "가중치 a와 각 단어의 벅테 $h_s$로 부터 가중치 합을 구하여 우리가 원하는 벡터를 얻는다.\n",
    "\n",
    "\n",
    "![](./img/IMG_E5D88A78F981-1.jpeg)\n",
    "\n",
    "\n",
    "맥갉 벡터 c에는 \"나\" 벡터의 성분이 많이 들어있다. 즉, \"나\" 벡토를 '선택'하는 작업을 이 가중합으로 데체하고 있다고 할 수 있다.\n",
    "\n",
    "맥락벡터 c에는 현 시간의 변환을 수행하는데 필요한 정보가 담겨 있다. 더 정확하게 말하면 그렇게 되도록 데이터로 부터 학습하는 것이다.\n",
    "\n",
    "각 단어의 중요도를 나타내는 가중치가 a가 있다면, 가중합을 이용해 '맥락 멕터'를 얻을 수 있다.\n",
    "\n",
    "\n",
    "![](./img/IMG_7F1E3B077795-1.jpeg)\n",
    "\n",
    "\n",
    "Decoder의 LSTM 계층의 은닉상태 벡터를 h라 했다. 지금 목표는 h가 $h_s$의 각 단어와 얼마나 비슷한가를 수치로 나타내는 것이다.\n",
    "\n",
    "그 직관적인 의미는 두 벡터가 얼마나 같은 방향으로 향하고 있는가 이다. 유사도를 표현하는 척도로 내적을 이용하는 것은 자연스러운 선택이다.\n",
    "\n",
    "![](./img/IMG_AE5720082D10-1.jpeg)\n",
    "\n",
    "\n",
    "벡터의 내적을 이용해 h와 $h_s$의 각 단어 벡터와 유사도를 구한다. 그리고 s는 그결과이다. s는 정규화 하기전에 값이며 'score' 라고도 한다. s를 정규화 하기 위해 softmax를 적용한다.\n",
    "\n",
    "![](./img/IMG_880968EE1953-1.jpeg)\n",
    "\n",
    "\n",
    "맥락 벡터를 계산하는 계산 그래프\n",
    "\n",
    "![](./img/IMG_70594EFE167C-1.jpeg)\n",
    "\n",
    "\n",
    "계산을 weight sum 계층과 Attension weight 계층 2개로 나눠 구현했다. 이 계산에 따르면 Attension weight 계층은 Encoder가 출력하는 각 단어의 벡터 $h_s$\n",
    "에 주목하여 해당 단어의 가중치 a를 구한다.\n",
    "이어서 weight sum 계층이 a와 $h_s$의 가중합을 구하고 그 결과를 맥락벡터로 c로 출력한다. 이러한 계산을 수행하는 계층을 Attension 계층이라고 부른다.\n",
    "\n",
    "\n",
    "![](./img/IMG_748A83C3A7D9-1.jpeg)\n",
    "\n",
    "## CODE\n",
    "\n",
    "### 파이썬으로 RNN 구현하기\n",
    "\n",
    "직접 Numpy로 RNN층을 구현해보겠다. 앞서 메모리 셀에서 은닉 상태를 계산하는 식을 다음과 같이 정의했다.\n",
    "\n",
    "$$ h_t = tanh(W_x X_t + W_h h_{t-1} + b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "timesteps = 10 # 시점의 수. NLP에서는 보통 문장의 길이가 된다.\n",
    "input_size = 4 # 입력의 차원. NLP에서는 보통 단어 벡터의 차원이 된다.\n",
    "hidden_size = 8 # 은닉 상태의 크기. 메모리 셀의 용량이다.\n",
    "\n",
    "inputs = np.random.random((timesteps, input_size)) # 입력에 해당되는 2D 텐서\n",
    "\n",
    "hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태는 0(벡터)로 초기화\n",
    "# 은닉 상태의 크기 hidden_size로 은닉 상태를 만듬."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(hidden_state_t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 8)\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "Wx = np.random.random((hidden_size, input_size))  # (8, 4)크기의 2D 텐서 생성. 입력에 대한 가중치.\n",
    "Wh = np.random.random((hidden_size, hidden_size)) # (8, 8)크기의 2D 텐서 생성. 은닉 상태에 대한 가중치.\n",
    "b = np.random.random((hidden_size,)) # (8,)크기의 1D 텐서 생성. 이 값은 편향(bias).\n",
    "\n",
    "print(np.shape(Wx))\n",
    "print(np.shape(Wh))\n",
    "print(np.shape(b))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(2, 8)\n",
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n",
      "(8, 8)\n",
      "(9, 8)\n",
      "(10, 8)\n",
      "[[0.71196244 0.86376132 0.7790863  0.90523735 0.91636757 0.67096679\n",
      "  0.82222076 0.74150296]\n",
      " [0.99984533 0.99983148 0.99994788 0.99992894 0.99994693 0.99973296\n",
      "  0.99982403 0.99999549]\n",
      " [0.99992331 0.99994058 0.99999267 0.99997304 0.9999802  0.9999357\n",
      "  0.99992476 0.99999897]\n",
      " [0.99999043 0.99998451 0.99999683 0.99999562 0.99999601 0.99998622\n",
      "  0.99997633 0.99999979]\n",
      " [0.99992323 0.99987017 0.99999238 0.99997125 0.99998015 0.9998839\n",
      "  0.99990372 0.99999886]\n",
      " [0.99997817 0.9999703  0.99999454 0.99998906 0.99999239 0.99996594\n",
      "  0.99996463 0.99999966]\n",
      " [0.99998714 0.99998964 0.99999705 0.99999564 0.99999115 0.99998901\n",
      "  0.99997314 0.99999974]\n",
      " [0.99998532 0.99996633 0.99999563 0.99999235 0.99999567 0.99997089\n",
      "  0.99996791 0.99999971]\n",
      " [0.99998762 0.99997967 0.99999658 0.9999949  0.99999233 0.99998007\n",
      "  0.9999696  0.99999975]\n",
      " [0.99999609 0.9999944  0.99999794 0.99999824 0.99999687 0.99999448\n",
      "  0.99998641 0.9999999 ]]\n"
     ]
    }
   ],
   "source": [
    "total_hidden_states = []\n",
    "\n",
    "# 메모리 셀 동작\n",
    "for input_t in inputs: # 각 시점에 따라서 입력값이 입력됨.\n",
    "  output_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b) # Wx * Xt + Wh * Ht-1 + b(bias)\n",
    "  total_hidden_states.append(list(output_t)) # 각 시점의 은닉 상태의 값을 계속해서 축적\n",
    "  print(np.shape(total_hidden_states)) # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)\n",
    "  hidden_state_t = output_t\n",
    "\n",
    "total_hidden_states = np.stack(total_hidden_states, axis = 0)\n",
    "# 출력 시 값을 깔끔하게 해준다.\n",
    "\n",
    "print(total_hidden_states) # (timesteps, output_dim)의 크기. 이 경우 (10, 8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 파이토치의 nn.RNN()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "입력 크기와 은닉 상태의 크기를 정의한다. 은닉 상태의 크기는 대표적인 RNN의 하이퍼파라미터이다. 여기서 입력의 크기는 매 시점마다 들어가는 입력의 크기를 의미한다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "hidden_size = 8\n",
    "\n",
    "# (batch_size, time_steps, input_size)\n",
    "\n",
    "inputs = torch.Tensor(1, 10, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "nn.RNN()을 사용하여 RNN의 셀을 만듭니다. 인자로 입력의 크기, 은닉 상태의 크기를 정의해주고, batch_first=True를 통해서 입력 텐서의 첫번째 차원이 배치 크기임을 알려줍니다.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "cell = nn.RNN(input_size, hidden_size, batch_first=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "입력 텐서를 RNN 셀에 입력하여 출력을 확인해본다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "outputs, _status = cell(inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.shape) # 모든 time-step의 hidden_state\n",
    "\n",
    "print(_status.shape) # (층의 개수, 배치 크기, 은닉 상태의 크기)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 양방향 순환 신경망(Bidirectional Recurrent Neural Network)\n",
    "\n",
    "\n",
    "양방향 순환 신경망은 시점 t에서의 출력값을 예측할 때 이전 시점의 데이터뿐만 아니라, 이후 데이터로도 예측할 수 있다는 아이디어에 기반합니다.\n",
    "\n",
    "Exercise is very effective at [          ] belly fat.\n",
    "\n",
    "1) reducing\n",
    "\n",
    "2) increasing\n",
    "\n",
    "3) multiplying\n",
    "\n",
    "\n",
    "'운동은 복부 지방을 [ ] 효과적이다'라는 영어 문장이고, 정답은 reducing(줄이는 것)입니다. 그런데 위의 영어 빈 칸 채우기 문제를 잘 생각해보면 정답을 찾기 위해서는 이전에 나온 단어들만으로는 부족합니다. 목적어인 belly fat(복부 지방)를 모르는 상태라면 정답을 결정하기가 어렵습니다.\n",
    "\n",
    "\n",
    "양방향 RNN은 하나의 출력값을 예측하기 위해 기본적으로 두 개의 메모리 셀을 사용합니다. 첫번째 메모리 셀은 앞에서 배운 것처럼 앞 시점의 은닉 상태(Forward States)를 전달받아 현재의 은닉 상태를 계산합니다. 위의 그림에서는 주황색 메모리 셀에 해당됩니다. 두번째 메모리 셀은 앞에서 배운 것과는 다릅니다. 앞 시점의 은닉 상태가 아니라 뒤 시점의 은닉 상태(Backward States)를 전달 받아 현재의 은닉 상태를 계산합니다. 위의 그림에서는 초록색 메모리 셀에 해당됩니다. 그리고 이 두 개의 값 모두가 출력층에서 출력값을 예측하기 위해 사용됩니다.\n",
    "\n",
    "물론, 양방향 RNN도 다수의 은닉층을 가질 수 있습니다. 아래의 그림은 양방향 순환 신경망에서 은닉층이 1개 더 추가되어 은닉층이 2개인 깊은(deep) 양방향 순환 신경망의 모습을 보여줍니다.\n",
    "\n",
    "\n",
    "![](https://wikidocs.net/images/page/22886/rnn_image6_ver3.PNG)\n",
    "\n",
    "\n",
    "다른 인공 신경망 모델들도 마찬가지이지만, 은닉층을 무조건 추가한다고 해서 모델의 성능이 좋아지는 것은 아닙니다. 은닉층을 추가하면, 학습할 수 있는 양이 많아지지만 또한 반대로 훈련 데이터 또한 그만큼 많이 필요합니다.\n",
    "\n",
    "양방향 순환 신경망을 파이토치로 구현할 때는 nn.RNN()의 인자인 bidirectional에 값을 True로 전달하면 됩니다. 이번에는 층이 2개인 깊은 순환 신경망이면서 양방향인 경우, 앞서 실습했던 임의의 입력에 대해서 출력이 어떻게 달라지는지 확인해봅시다."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 16])\n",
      "torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# (batch_size, time_steps, input_size)\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True, bidirectional = True)\n",
    "\n",
    "\n",
    "outputs, _status = cell(inputs)\n",
    "\n",
    "print(outputs.shape)\n",
    "\n",
    "print(_status.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "첫번째 리턴값의 크기는 단뱡 RNN 셀 때보다 은닉 상태의 크기의 값이 두 배가 되었습니다. 여기서는 (배치 크기, 시퀀스 길이, 은닉 상태의 크기 x 2)의 크기를 가집니다. 이는 양방향의 은닉 상태 값들이 연결(concatenate)되었기 때문입니다.\n",
    "\n",
    "두번째 리턴값의 크기는 (층의 개수 x 2, 배치 크기, 은닉 상태의 크기)를 가집니다. 이는 정방향 기준으로는 마지막 시점에 해당되면서, 역방향 기준에서는 첫번째 시점에 해당되는 시점의 출력값을 층의 개수만큼 쌓아 올린 결과값입니다.\n",
    "\n",
    "\n",
    "### 문자 단위 RNN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n"
     ]
    }
   ],
   "source": [
    "input_str = 'apple'\n",
    "label_str = 'pple!'\n",
    "char_vocab = sorted(list(set(input_str+label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "print ('문자 집합의 크기 : {}'.format(vocab_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n",
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "source": [
    "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
    "print(char_to_index)\n",
    "index_to_char={}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "print(index_to_char)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 3, 2]\n",
      "[4, 4, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data)\n",
    "print(y_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 4, 3, 2]]\n",
      "[[4, 4, 3, 2, 0]]\n",
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "# 배치 차원 추가\n",
    "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
    "x_data = [x_data]\n",
    "y_data = [y_data]\n",
    "print(x_data)\n",
    "print(y_data)\n",
    "\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "X = torch.FloatTensor(x_one_hot)\n",
    "Y = torch.LongTensor(y_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
      "레이블의 크기 : torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
    "print('레이블의 크기 : {}'.format(Y.shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "\n",
    "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(input_size, hidden_size, output_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "outputs = net(X)\n",
    "print(outputs.shape) #"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.4896806478500366 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
      "1 loss:  1.2946032285690308 prediction:  [[4 4 4 4 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppp!\n",
      "2 loss:  1.123014211654663 prediction:  [[4 4 4 4 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppp!\n",
      "3 loss:  0.9894015192985535 prediction:  [[4 4 4 4 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppp!\n",
      "4 loss:  0.8273143768310547 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "5 loss:  0.7310376763343811 prediction:  [[4 4 4 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppp!!\n",
      "6 loss:  0.6336865425109863 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "7 loss:  0.5406838655471802 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "8 loss:  0.4798303544521332 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "9 loss:  0.431610643863678 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "10 loss:  0.3813413977622986 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "11 loss:  0.33007216453552246 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "12 loss:  0.2777702808380127 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "13 loss:  0.22711117565631866 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "14 loss:  0.18109911680221558 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "15 loss:  0.14050427079200745 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "16 loss:  0.10513667017221451 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "17 loss:  0.07615766674280167 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "18 loss:  0.053728751838207245 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "19 loss:  0.037270523607730865 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "20 loss:  0.02597823180258274 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "21 loss:  0.018294617533683777 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "22 loss:  0.013124704360961914 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "23 loss:  0.009659252129495144 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "24 loss:  0.007305465638637543 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "25 loss:  0.005677626933902502 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "26 loss:  0.004531539976596832 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "27 loss:  0.0037096806336194277 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "28 loss:  0.0031089126132428646 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "29 loss:  0.00266107264906168 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "30 loss:  0.002320670522749424 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "31 loss:  0.0020570470951497555 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "32 loss:  0.0018492210656404495 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "33 loss:  0.0016828114166855812 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "34 loss:  0.001547392108477652 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "35 loss:  0.0014357392210513353 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "36 loss:  0.0013425517827272415 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "37 loss:  0.001263858051970601 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "38 loss:  0.0011967800091952085 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "39 loss:  0.0011391049483790994 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "40 loss:  0.0010889768600463867 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "41 loss:  0.0010451104026287794 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "42 loss:  0.0010064822854474187 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "43 loss:  0.0009722354006953537 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "44 loss:  0.0009416555985808372 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "45 loss:  0.000914266798645258 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "46 loss:  0.0008895213832147419 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "47 loss:  0.000867085880599916 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "48 loss:  0.000846769951749593 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "49 loss:  0.0008281448972411454 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "50 loss:  0.0008110441267490387 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "51 loss:  0.0007952769519761205 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "52 loss:  0.0007807720685377717 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "53 loss:  0.0007672674837522209 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "54 loss:  0.0007546917768195271 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "55 loss:  0.0007429496035911143 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "56 loss:  0.0007320172153413296 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "57 loss:  0.0007217517122626305 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "58 loss:  0.0007119864458218217 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "59 loss:  0.0007028641412034631 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "60 loss:  0.0006941944593563676 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "61 loss:  0.0006859534187242389 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "62 loss:  0.0006782125565223396 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "63 loss:  0.0006707098218612373 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "64 loss:  0.0006636358448304236 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "65 loss:  0.0006567761301994324 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "66 loss:  0.0006503214244730771 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "67 loss:  0.0006440334254875779 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "68 loss:  0.0006380311097018421 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "69 loss:  0.0006321955588646233 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "70 loss:  0.000626622058916837 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "71 loss:  0.000621215149294585 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "72 loss:  0.0006159988697618246 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "73 loss:  0.0006108538946136832 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "74 loss:  0.0006059472216293216 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "75 loss:  0.0006011119112372398 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "76 loss:  0.0005964910378679633 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "77 loss:  0.0005919176037423313 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "78 loss:  0.00058746337890625 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "79 loss:  0.0005831758608110249 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "80 loss:  0.0005788883427157998 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "81 loss:  0.0005747436662204564 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "82 loss:  0.0005706941592507064 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "83 loss:  0.0005667400546371937 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "84 loss:  0.0005628096405416727 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "85 loss:  0.0005590460496023297 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "86 loss:  0.0005553061491809785 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "87 loss:  0.0005516139208339155 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "88 loss:  0.0005480169202201068 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "89 loss:  0.0005444437847472727 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "90 loss:  0.0005409658188000321 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "91 loss:  0.0005374879110604525 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "92 loss:  0.0005340576171875 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "93 loss:  0.0005307224928401411 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "94 loss:  0.0005274350987747312 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "95 loss:  0.0005241475882939994 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "96 loss:  0.000520955421961844 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "97 loss:  0.0005178346764296293 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "98 loss:  0.0005146900657564402 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "99 loss:  0.0005116169340908527 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
    "\n",
    "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
    "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}