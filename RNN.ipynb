{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 순환 신경망(RNN)\n",
    "\n",
    "피드포워드(Feed Forward) : 흐름이 단방향인 신경망 피드포워드의 단점은 시계열 데이터를 잘 다루지 못한다.\n",
    "시계열 데이터의 성질을 충분히 학습할 수 없다. $\\rightarrow$ **순환신경망이 등장한 배경**\n",
    "\n",
    "\n",
    "### 언어모델\n",
    "언어 모델은 단어 나열에 확률을 부여한다. 특정한 단어 시퀀스에 대해서 그 시퀀스가 일어날 가능성이 어느정도인지 확률로 평가한다.\n",
    "\n",
    "단어가 $W_1, W_2, \\cdots, W_m$ 이라는 순서대로 출연한 확률을 $P(W_1, \\cdots, W_m)$으로 나타낸다. 이 확률을 여러 사건이 동시에 일어날 확률 이므로 동시 확률이라고 부른다.\n",
    "\n",
    "![](https://blog.kakaocdn.net/dn/dOYvMK/btqTSTuz1su/f1DzIk9u2nWUE3tY1eLVk1/img.png)\n",
    "\n",
    "곱셈 정리를 사용하여 m개의 단어의 동시확률 $P(W_1,\\cdots, W_m)$을 사후 확률로 나타낼수 있다.\n",
    "\n",
    "머신러닝이나 통계학에서 마르코프 연쇄 또는 마르코프 모델이라는 말을 자주 듣는다. 마르코프 연쇄란 미래의 상태가 현재 상태에만 의존해서 발생하는 것을 말한다.\n",
    "\n",
    "\n",
    "\n",
    "CBow 모델의 은닉층에서는 단어 벡터들이 더해지므로 맥락이 무시된다.\n",
    "\n",
    "|CBow|신경 확률적 모델|\n",
    "|---|---|\n",
    "|![](./img/IMG_24B8A9F7C428-1.jpeg)|![](./img/IMG_B1C83D272BDB-1.jpeg)|\n",
    "\n",
    "RNN은 맥락이 아무리 길더라도 그 맥의 정보를 기억하는 메커니즘을 갖추고 있다. word2vec은 단어의 분산표현을 얻을 목적으로 고안된 기법이다. 따라서 이를 언어 모델로 사용하진 않는다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### RNN(Recurrent Neural Network)\n",
    "\n",
    "\n",
    "\n",
    "순환하기 위해서는 닫힌 경로가 필요하다. 데이터가 정보를 끊임없이 갱신하다.\n",
    "\n",
    "<img src = \"https://blog.kakaocdn.net/dn/w9Wfk/btqTvRJdz8m/GL3aDzhlGB5ZKX0hGUMt30/img.png\">\n",
    "\n",
    "$x_t$(단어의 분산표현)를 입력받는다, t는 시각을 뜻한다.\n",
    "\n",
    "<img src = './img/IMG_71313BEE4FDE-1.jpeg'>\n",
    "\n",
    "\n",
    "다수의 RNN 계층 모두가 실제로는 '같은 계층'인 것이 지금까지의 신경망과는 다르다.\n",
    "\n",
    "각 시각의 RNN 계층은 그 계층으로의 입력과 1개의 RNN 계츨으로의 출력을 받는다. 두 정보를 바탕으로 현시각의 출력을 계산한다.\n",
    "\n",
    "$$h_t = tanh(h_{t-1} w_h + x_t w_x + b)$$\n",
    "\n",
    "RNN에는 가중치가 2개 있다. 하나는 입력 $x_t$를 h로 변환하기 위한 $w_x$, RNN 출력을 다음 시각의 출력으로 변환하기위한 기증치 $w_h$ 또한 편행 b\n",
    "$h_t$는 다른 계층을 향해 출력되는 동시에 다음 시각의 RNN 계층을 향해 오른쪽으로 출련된다.\n",
    "\n",
    "\n",
    "\n",
    "### BPTT\n",
    "\n",
    "\n",
    "RNN 계층은 가로로 펼쳐진 신경망으로 볼 수 있다.\n",
    "\n",
    "BPTT(BackPropagation Through Time) : 시간 방향으로 펼친 오차역전파법\n",
    "\n",
    "시계열 데이터의 시간 크기가 커지는 것에 비례해서 BPTT가 소비하는 컴퓨팅 자원도 증가한다. 시간 크기가 커지면 역전파시의 기울기가 불안정해진다.\n",
    "\n",
    "BPTT로 기울기를 구하려면, 매 시각 RNN 계츨의 중간 메모리를 유지해두지 않으면 안된다.\n",
    "\n",
    "큰 시계열 데이터를 처리할 때, 신경망 연결을 적당한 길이로 끊는다.\n",
    "\n",
    "Truncated BPTT: 너무 길어진 신경망을 적당한 길이로 자르고 잘라낸 신경망을 오차역전파법을 수행한다.\n",
    "\n",
    "역전파의 연결만 끊는다 순전파의 연결을 유지한다.\n",
    "RNN에서 Truncated BPTT를 수행할떄는 데이터를 순서대로 입력해야한다.\n",
    "\n",
    "<img src = \"./img/IMG_19D478395C38-1.jpeg\">\n",
    "\n",
    "\n",
    "Time RNN 계층은 T개의 RNN 계층으로 구성한다.\n",
    "![](./img/IMG_5316F7DBE529-1.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### RNNLM(RNN Language Model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](./img/IMG_02D8985287CE-1.jpeg)\n",
    "\n",
    "첫번째 층을 Embedding 계층이다. 이 계층은 단어 ID를 단어의 분산 표현으로 변환한다. 그 분산 표현은 RNN 계층으로 입력된다.\n",
    "RNN 계층은 은닉 상태를 다음 층으로 출력함과 동시에 다음 시각의 RNN 계층\n",
    "으로 출력한다. 그리고 RNN 계층이 위로 출력한 은닉상태는 Affine 계층을 거쳐 softmax 계층으로 이어진다.\n",
    "\n",
    "RNNLM은 지금까지 입력된 다어를 '기억'하고 그것을 바탕으로 다음에 출현할 단어를 예측한다.\n",
    "RNN 게층이 과거에서 현재로 데이터를 계속 흘려주며서 과거의 정보를 인코딩해 저장할 수 있는 것이다.\n",
    "\n",
    "\n",
    "\n",
    "### 언어 모델의 평가\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "언어 모델은 주어진 과거 단어로 부터 다음에 출현할 단어의 확률 분포를 출력한다.\n",
    "언어 모델의 성능을 평가하는 척도로 퍼플렉시티(perplexity)를 자주 이용한다.\n",
    "**퍼플렉시티는 확률의 역수이다.**\n",
    "\n",
    "![](./img/IMG_1D1373B31FA3-1.jpeg)\n",
    "\n",
    "perplexity는 작을수록 좋다.\n",
    "\n",
    "- 이 값은 분기수로 이해할 수 있다. 분기수란 다음에 취할 수 있는 선택사항 수. 분기수가 1.25라는 것은 다음에 후보로 출연할 단어가 1개 정도로 좁혀졌다는 의미이다.\n",
    "\n",
    "- 입력데이터가 여러개 일떄의 perplexity $\\rightarrow L=-\\frac{1}{N} \\sum_n \\sum_k t_{nk} log(y_{nk})$\n",
    "- $perplexity = e^L$\n",
    "\n",
    "\n",
    "## 게이트가 추가된 RNN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "베이직한 RNN은 구조가 단순하여 구현은 쉽지만 성능이 좋지 못하다. 그 원인은 시계열 데이터에서 시간적으로 멀리 떨어진, 장기 Long term 의존 관계를 잘 학습할 수 없다.\n",
    "\n",
    "LSTM이나 GRU에는 '게이트'라는 구조가 더해져 있는데. 이 게이트 덕분에 시계열 데이터의 장기 의존 관계를 학습할 수 있다.\n",
    "\n",
    "베이직한 RNN의 문제점은 BPTT에서 기울기 소실 혹은 기울기 폭발이 일어나기 때문이다.\n",
    "- 시간 방향의 기울기에 주목하면 역전파로 전해지는 기울기는 차례로 'tanh', '+', 'matmul' 연산을 통과하게 된다. '+'의 역전파는 상류에서 전해지는 기울기를 하류로 흘려보낼 뿐이다.\n",
    "\n",
    "![](https://www.tutorialexample.com/wp-content/uploads/2020/08/the-graph-of-tanhx-function-derivative.png)\n",
    "\n",
    "$\\frac{dy}{dx}$의 값이 0~1 사이 즉 역전파에서 기울기가 tanh 노드를 지날떄 마나 값이 작아진다.\n",
    "\n",
    "Matmul 노드에서의 역전파는 $dhw_r^t$라는 행렬곱으로 기울기를 계산한다. 그리고 같은 시계열의 데이터 크기 만큼 반복한다. 여기서 주목할 점은 매번 똑같은 가중치인 $w_r$가 사용된다는 것이다.\n",
    "\n",
    "$w_r$가 1보다 크면 기울기가 지수적으로 증가하고, 1보다 작으면 지수적으로 감소한다.\n",
    "\n",
    "기울기 폭발의 대책으로는 전통적인 기법이 있다.\n",
    "\n",
    "**기울기 클리핑(gradient clipping)**\n",
    "\n",
    "if $$\\lVert \\hat g \\rVert \\geq threshold: \\hat g = \\frac{threshold}{\\lVert \\hat g \\rVert}\\hat g$$\n",
    "\n",
    "\n",
    "#### LSTM의 인터페이스\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/1024/0*243z3lyg05TswC9D)\n",
    "\n",
    "\n",
    "\n",
    "LSTM 계층에는 c라는 경로가 있다. c를 기억셀이라 하며 LSTM의 기억 매커니즘이다.\n",
    "\n",
    "기억셀의 특징은 데이터를 자기자신하고만(LSTM 게층 내에서만) 주고 받는다는 것이다. 즉, LSTM 계층내에서만 완결되고, 다른 계층으로는 출력하지 않는다.\n",
    "\n",
    "LSTM의 출력은 은닉벡터 h 뿐이다. 기억셀 c는 외부에서는 보이지 않는다.\n",
    "\n",
    "$c_t$에는 과거로부터  시작 t까지에 필요한 모든 정보가 저장되어있다고 가정한다.\n",
    "기억셀 $c_t$는 3개의 입력 $(c_{t-1}, h_{t-1}, x_t)$으로 부터 '어떤 계산'을 수행하여 구할 수 있다.\n",
    "\n",
    "$h_t = tanh(c_t)$이다. 이는 $c_t$의 각 요소에 tanh 함수를 적용한다는 뜻이다.\n",
    "\n",
    "기억셀 $c_t$와 은닉상태 $h_t$의 원소수는 같다.\n",
    "\n",
    "게이트는 데이터의 흐름을 제어한다.\n",
    "LSTM에서 사용하는 게이트는 '열기/닫기' 뿐만 아니라 어느정도 열지를 조절할 수 있다. 어느정도를 열림상태 openness라고 부른다.\n",
    "\n",
    "\n",
    "\n",
    "![](./img/IMG_49CEF7B36468-1.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "게이틀의 열림 상태는 0.0 ~ 0.1 사이의 실수로 나타낸다. 게이트는 게이트의 연림 상태를 제어하기 위해서 전용 가중치 매개변수를 이용하며, 이 가중치는 학습데이터로부터 갱신된다.\n",
    "열림 상태를 구할 때는 시그모이드 함수를 사용한다.\n",
    "\n",
    "#### output 게이트\n",
    "\n",
    "\n",
    "\n",
    "이 게이트는 다음 은닉 상태 $h_t$의 출력을 담당하는 게이트이므로 output 게이트라고한다.\n",
    "\n",
    "$$ O = \\sigma(x_t W^{(o)}_x + h_{t-1} W^{(o)}_h + b^{(o)})$$\n",
    "\n",
    "\n",
    "\n",
    "![](./img/IMG_E358232B139E-1.jpeg)\n",
    "\n",
    "\n",
    "\n",
    "$\\sigma$의 출력을 O라고 하면 $h_t$ O와 $tanh(c_t)$의 곱으로 계산된다. 여기서 말하는 곱은 원소별 곱이며 이것을 아다마르 곱이라고도 한다.\n",
    "\n",
    "$$h_t = O \\odot tanh(c_t)$$\n",
    "\n",
    "\n",
    "\n",
    "#### forget 게이트\n",
    "\n",
    "\n",
    "\n",
    "$c_{t-1}$의 기억중에 불필요한 기억을 잊게 해주는 게이트\n",
    "\n",
    "![](./img/IMG_E16A2B42649C-1.jpeg)\n",
    "\n",
    "$$f = \\sigma(x_t W_x^{(f)} + h_{t-1}W^f_h + b^f)$$\n",
    "\n",
    "$$c_t = f \\odot c_{t-1}$$\n",
    "\n",
    "![](./img/IMG_A0CF4CA5383A-1.jpeg)\n",
    "\n",
    "\n",
    "#### tanh 노드\n",
    "\n",
    "\n",
    "tanh 노드가 계산한 결과가 이전 시각의 기억셀 $c_{t-1}$에 더해진다. 기억셀에 새로운 정보가 추가된 것이다. 이 tanh 노드는 게이트가 아니며 새로운 정보를 기억셀에\n",
    "추가하는 것이 목적이다. 활성화 함수로 sigmoid가 아닌 tanh 함수를 사용한다.\n",
    "\n",
    "$$g = tanh(x_tW^{(g)}_x + h_{t-1}W_h^{(g)} + b^{(g)})$$\n",
    "\n",
    "\n",
    "#### input 게이트\n",
    "\n",
    "\n",
    "\n",
    "![](./img/IMG_F2889C479F30-1.jpeg)\n",
    "\n",
    "\n",
    "input gate는 g의 각 원소가 서로 추가되는 정보로써 가치가 얼마나 큰지 판단한다. 다른 과넞ㅁ에서 보면 input gate에 의해 가중된 정보를 추가하는 것이다.\n",
    "\n",
    "$$i = \\sigma(x_t W_x^{(i)} + h_{t-1}W^{(i)}_h + b^{(i)})$$\n",
    "\n",
    "\n",
    "\n",
    "### LSTM의 기울기 흐름\n",
    "\n",
    "\n",
    "\n",
    "기억셀의 역전파에서는 '+', 'x' 노드만 지나게 된다. '+'노드는 상류의 흐름을 그대로 하류에 넘겨준다. 또한 $'\\times'$노드는 행렬곱이 아닌 원소별곱(아다마르 곱)\n",
    "을 계산한다. **매 시각 다른 게이트 값을 이용해 원소별 곱을 계산한다.** 새로운 게이트 값을 이용하므로 곱셈의 효과가 누적되지 않아 기울기 손실이 발생하기 어렵게 된다.\n",
    "\n",
    "$'\\times'$ 노드의 계산은 forget 게이트가 제어한다. forget gate가 '잊어야 한다' 고 판단한 기억셀의 원소는 기울기가 작아지고 반대의 경우에는 기울기가 약화되지 않은 채로 과거로 전해진다.\n",
    "\n",
    "\n",
    "### LSTM 계층의 다각화\n",
    "\n",
    "\n",
    "\n",
    "![](./img/IMG_5594A564917C-1.jpeg)\n",
    "\n",
    "LSTM 계층을 2층, 3층 식으로 어려겹 쌓으면 언어 모델의 정확도가 향상되리라 기대할 수 있다.\n",
    "\n",
    "\n",
    "### 과적합 억제\n",
    "\n",
    "\n",
    "\n",
    "#### 드롭아웃에 의한 과적합 억제\n",
    "\n",
    "\n",
    "층을 깊게 쌓음으로써 표현력이 풍부한 모델을 만들 수 있다. 그러나 이런 모델은 종종 overfitting을 일으킨다.\n",
    "\n",
    "과적합을 억제하기 위한 전통적인 방법으로 훈련 데이터양 늘리기, 모델의 복잡도 줄이기 그 외에는 모델의 복잡도에 패널티를 주는 정규화가 효과적이다.\n",
    "\n",
    "드롭아웃은 무작위로 뉴런을 선택하여 선택한 뉴런을 무시한다 RNN에서 시계열 방향으롣 드롭아웃을 넣어버리면 시간이 흐름에 따라 정보가 사라질 수 있다. 즉, 흐르는 시간에 비례해서 드롭아웃에 의한 노이즈가 축적된다.\n",
    "\n",
    "\n",
    "![](./img/IMG_4C5476E6FDC2-1.jpeg)\n",
    "\n",
    "\n",
    "이렇게 구성하면 시간 방향으로 아무리 진행해도 정보를 잃지 않는다.\n",
    "\n",
    "변형 드롭아웃을 통해서 시간방향으로 적용하는데 성공했다. 변형 드롭아웃은 깊이 방향은 물론이고 시간뱡향으로도 이용할 수 있다. 같은 계층에 속한 드롭아웃들은 같은 마스크(mask)\n",
    "를 공유한다. 마스크란 통과/차단을 결정하는 이진 형태의 무작위 패천이다.\n",
    "\n",
    "![](./img/IMG_76BFA8F0A310-1.jpeg)\n",
    "\n",
    "같은 계층의 드롭아웃끼리 마스크를 공유함으로써 마스크가 '고정'된다. 그 결과 정보를 읽게 되는 방법도 '고정' 되므로 일번적인 드롭아웃과 달리 지수적 손실되는 사태를 피할 수 있다.\n",
    "\n",
    "\n",
    "#### 가중치 공유\n",
    "\n",
    "\n",
    "언어 모델을 계선하는 간단한 트릭중에서 가중치 공유가 있다.\n",
    "\n",
    "![](./img/IMG_86DC3A83F5A2-1.jpeg)\n",
    "\n",
    "두 계층의 가중치를 공유함으로써 학습하는 매개변수 수가 크게 줄어드는 동시에 정확도가 향상되는 일석이조의 기술이다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}