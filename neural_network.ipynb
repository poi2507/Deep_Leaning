{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 신경망\n",
    "\n",
    "퍼셉트론으로 복잡한 함수도 표현이 가능해졌다. 하지만 가중치를 설정하는 작업은 여전히 사람이 수동으로 한다.\n",
    "\n",
    "신경망은 가중치 매개변수의 적절한 값을 데이터로부터 자동으로 학습하는 능력이 신경망의 중요한 성질이다.\n",
    "\n",
    "![](data:image/png;base64,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)\n",
    "\n",
    "### 활성화 함수의 등장\n",
    "\n",
    "입력 신호의 총합을 출력 신호로 변환하는 함수를 활성화 함수(activation function)라고 한다. 활성화 함수는 입력신호의 총합이 활성화를 일으키는지 정하는 역할을 한다.\n",
    "\n",
    "### 시그모이드 함수\n",
    "\n",
    "신경망에서 자주 사용되는 활성화 함수인 시그모이드(sigmoid) 함스를 나타내는 식이다\n",
    "- $h(x) = \\frac{1}{1+e^{-1}}$\n",
    "\n",
    "신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하고 그 변환된 신호를 다음 뉴런에 전달한다.\n",
    "\n",
    "### 계단 함수와 시그모이드 함수\n",
    "\n",
    "![](./NLP_1/img/IMG_5981A792CFAB-1.jpeg)\n",
    "\n",
    "\n",
    "그래프를 보고 느껴지는 점은 '매끄러움'의 차이이다. 시그모이드 함수는 부드러운 곡선이며 압력에 따라 출력이 연속적으로 변화한다.\n",
    "- 신경망에서는 연속적인 실수가 흐른다.\n",
    "\n",
    "### 비선형 함수\n",
    "신경망에서는 활성화 함수로 비선형 함수를 사용해야 한다.\n",
    "선형함수를 이용하면 **신경망의 층을 깊게 하는 의미가 없어진다.** 선형 함수의 문제는 층을 아무리 깊게 해도 '은닉층이 없는 네트워크' 로도 똑같이 기능을\n",
    "할 수 있다는데있다.\n",
    "\n",
    "선형함수인 $h(x) = cx$를 활성화 함수로 3층 네트워크를 만든다면 $y(x) = h(h(h(x)))$가 된다. 계산은\n",
    "$y(x) = c \\times c \\times c \\times x \\rightarrow y(x) = ax$ 와 같은 식이다. $a = c^3$으로 치환해주면 된다.\n",
    "\n",
    "\n",
    "![](data:image/png;base64,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)\n",
    "\n",
    "$h(x) = \\Bigg\\{\\frac{x (x> 0)}{0(x\\leq0)}  \\Bigg\\}$\n",
    "\n",
    "\n",
    "### 춣력층 설계하기\n",
    "\n",
    "신경망은 분류와 회귀 모두에 이용할 수 있다 다만 둘 중 어떤 문제냐에 따라 출력츨이 사용하는 함수가 달라진다 일반적으로 회귀에는 함등 함수를, 분류에는 소프트맥스를 사용한다.\n",
    "\n",
    "항등함수는 입력과 출력이 항상 같다는 뜻의 함등이다.\n",
    "\n",
    "![](/Users/taewoong/Documents/NLP_1/img/IMG_5EBE0E1440C4-1.jpeg)\n",
    "\n",
    "SoftMax → $y_k = \\frac{e^{a_k}}{\\sum_{i=1}^n e^{a_i}}$\n",
    "\n",
    "n은 출력층의 뉴런수, $y_k$는 그 중 k 번째 출력임을 뜻한다. 소프트맥스 함수의 분자는 입력신호 $a_k$의 지수함수, 분모는 모든 입력신호가 지수함수의 합으로 구성된다.\n",
    "\n",
    "소프트맥스 함수의 출력을 '확률'로 해석할 수 있다. 소프트맥스의 출력의 총합은 1이다. 소프트맥스를 적용해도 원소의 대소관계는 변하지 않는다. 이는 지수함수\n",
    "$y = e^x$가 단조 증가 함수이기 때문이다.\n",
    "\n",
    "신경망을 이용한 분류에서는 일반적으로 가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식한다.\n",
    "\n",
    "기계학습의 문제 풀이는 학숩과 추론의 두 단계를 거쳐 이뤄진다. 학습단계에서 모델을 학습하고 추론 단계에서 앞서 학습한 모델을 미지의 데이터에 더해서 추론을 수행한다.\n",
    "\n",
    "## 신경망 학습\n",
    "\n",
    "학습이란 훈련데이터로부터 가중치 메개변수의 최적값을 자동으로 획득하는 것을 뜻한다.\n",
    "\n",
    "### 데이터 주도 학습\n",
    "\n",
    "기계학습에서는 사람의 개입을 최소화하고 수집한 데이터로 부터 패턴을 찾으려 시도한다.\n",
    "\n",
    "딥러닝을 종단간 기계학습(end to end machine learning)이라고 부른다. 데이터에서 결과를 사람의 개입없이 얻는다는 뜻이다.\n",
    "\n",
    "### 손실함수\n",
    "\n",
    "신경망도 '하나의 지표'를 기준으로 최적의 매개변수 값을 탐색한다. 신경망 학습에서 사용하는 지표는 손실함수(loss function)라고 한다.\n",
    "\n",
    "오차제곱합 → $E = \\frac{1}{2} \\sum_k(y_k - t_k)^2$\n",
    "\n",
    "교차엔트로피 오차 → $E = -\\sum_k t_k log(y_k)$\n",
    "\n",
    "실질적으로 정답일때의 추정의 자연로그를 계산한다.\n",
    "교차엔트로피 오차는 정답일때의 출력이 전제 값을 정하게 된다.\n",
    "\n",
    "### 미니배치 학습\n",
    "\n",
    "$E = -\\frac{1}{N}\\sum_n\\sum_k t_{nk} log(y_{nk})$\n",
    "\n",
    "N으로 나누어 정규화하고 있다. N으로 나눔으로써 '평균 손실 함수'를 구한다.\n",
    "\n",
    "왜 손실할수를 설정하는가?\n",
    "- 가중치 매개변수의 손실함수 미분이란 '가중치 매개변수값을 아주 조금씩 변화 시켰을 때, 손실함수가 어떻게 변하나' 라는 의미이다.\n",
    "- 신경망을 학습할 때 정확동를 지표로 삼으면 안된다. 정확도를 지표로 하면 미분이 대부분의 장소에서 0이기 때문이다.\n",
    "\n",
    "\n",
    "### 활성화 함수 계층 구하기\n",
    "\n",
    "\n",
    "|ReLU|Sigmoid|\n",
    "|--|--|\n",
    "|![](https://blog.kakaocdn.net/dn/kZM5C/btqzhhOCX7u/fcWgVK4NDONy0EzhI4GFvK/img.png)|![](https://blog.kakaocdn.net/dn/Tbcdv/btqCqOiIovB/TPqwip21DxOpC70hSLjtCk/img.png)|\n",
    "|순전파일때 입력의 x가 0보다 크면 역전파는 상류 값을 그대로 하류로 흘려보낸다.반면, 순전파 때 x가 0이하면 하류로 신호를 보내지 않는다.||\n",
    "\n",
    "### 매개변수 갱신\n",
    "신경망 학습의 목적은 손실함수의 값을 가능한 낮추는 매개변수를 찾는 것이다. 이는 곧 매개변수의 최적값을 탐색하는 문제이며 이러한 문제를 푸는 것을 **최적화**라 한다.\n",
    "\n",
    "### 확률적 경사 하강법\n",
    "\n",
    "$W \\leftarrow W - \\eta\\frac{\\delta L}{\\delta w} $\n",
    "\n",
    "W는 갱신할 가중치 매개변수고  $\\frac{\\delta L}{\\delta w}$은 w에 대한 손실함수 기울기이다.\n",
    "\n",
    "![](./NLP_1/img/IMG_E137BCC93331-1.jpeg)\n",
    "\n",
    "SGD의 단점은 문제에 따라 비효율 적이다.\n",
    "\n",
    "비등방성 함수(방향에 따라 성질, 즉 여기에서는 기울기가 달라지는) 에서는 탐색경로가 비효율적이다.\n",
    "\n",
    "### 모멘텀\n",
    "\n",
    "모멘텀은 운동향을 뜻하는 단어로 물리와 관계가 있다.\n",
    "\n",
    "$v  \\leftarrow av - \\gamma\\frac{\\delta L}{\\delta w}$\n",
    "\n",
    "$W \\leftarrow W + v$\n",
    "\n",
    "v라는 새로운 변수가 나오는데, 이는 물리에서 말하는 속도에 해당한다. 기울기 방향으로 힘을 받아 물페가 가속된다는 물리법칙을 나타낸다.\n",
    "\n",
    "![](/Users/taewoong/Documents/NLP_1/img/IMG_6700C0047708-1.jpeg)\n",
    "\n",
    "SGD와 비교하면 지재그 정도가 덜한 것을 알수 있다. x축의 힘은 아주 작지만 방향이 변하지 않아서 일정하게 가속하기 때문이다. 거꾸로 y축의 힘은 크지만 위아래로 번갈아 가면서 상충하여 y축 방향의 속도는 안정적이지 않다.\n",
    "\n",
    "### AdaGrad\n",
    "\n",
    "신경망 학습에서는 학습률 값이 중요하다 이 값이 너무 작으면 학습 시간이 너무 길어지고 반대로 너무 크면\n",
    "방산하여 학습이 제대로 이뤄지지 않는다. 이 학습률을 정하는 효과적인 기술로 학습률 감소가 있다. 이는 학습을 진행하면서 학습률을 점차 줄여가는 방법이다.\n",
    "\n",
    "AdaGrad는 각각의 매개변수에 맞춤형 값을 만들어준다\n",
    "AdaGrad는 개별 매개변수에 적응형으로 학습률을 조정 하면서 학습을 진행한다.\n",
    "\n",
    "$h \\leftarrow h + \\frac{\\delta L}{\\delta W} \\odot \\frac{\\delta L}{\\delta W}$\n",
    "\n",
    "$W \\leftarrow W - \\eta \\frac{1}{\\sqrt{h}} \\frac{\\delta L}{\\delta W}$\n",
    "\n",
    "h는 기존 기울기 값을 제곱하여 계속 더해준다 그리고 매개변수를 갱신할때 $\\frac{1}{\\sqrt{h}}$을 곱해서 학습률을 조정한다. 매개변수의 원소 중에서 많이 움직인 원소는 학습률이 낮아진다는 뜻이다.\n",
    "\n",
    "AdaGrad는 과거의 기울기를 제곱하여 계속 더해간다 그래서 학습이 진행될 수 록 갱신 강도가 약해진다. 실제로 무한히 갱신하면 6순간 갱신량이 0이 되어서 전혀 갱신되지 않는다.\n",
    "이 문제를 개선한 기법으로 RMSProp라는 방법이 있다.\n",
    "RmsProp는 과거의 기울기를 균일하게 더해가는게 아니라 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 반영한다. 리르 지수 이동 평균이라 하며 과거의 기울기의 변영 규모를 기하급수적으로 감소시킨다.\n",
    "\n",
    "![](/Users/taewoong/Documents/NLP_1/img/IMG_9DD295F067B6-1.jpeg)\n",
    "\n",
    "최솟값을 향해 효쥴족으로 움직이는 것을 알 수 있다. y축 방향은 기울기가 커서 처음에는 움직이지만 큰 움직임에 비례해서 갱신 정도가 큰 폭으로 작아지도록 조정돤다.\n",
    "\n",
    "### Adam\n",
    "\n",
    "객관적으로 모멘텀과 AdaGrad를 융합한 듯한 방법이다.\n",
    "하이퍼파라미터가 편향 보정이 진행된다.\n",
    "\n",
    "![](/Users/taewoong/Documents/NLP_1/img/IMG_B4F46FD24D91-1.jpeg)\n",
    "\n",
    "Adam은 파라미터를 3개 설정한다. 하나는 지금까지의 학습률 나머지 2개는 일차모멘텀용 게수 $\\beta_1$와 이차 모멘텀용 계수 $\\beta_2$이다.\n",
    "\n",
    "![](/Users/taewoong/Documents/NLP_1/img/IMG_A5CDFB8EF77B-1.jpeg)\n",
    "\n",
    "\n",
    "### 가중치의 초기값\n",
    "가중치 감소는 단단히 말하자면 가중치 매개변수의 값이 작아지도록 학습하는 방법이다. 가중치의 값을 작게하여 오버피팅이 일어나지 않게하는 것이다.\n",
    "\n",
    "활성화 값들이 치우쳤다는 것은 표현력 관점에서는 큰 문제가 있다는 것이다. 다수의 뉴런이 거의 같은 값을 출력하고 있으니 뉴런을 여러개둔 의미가 없어진다.\n",
    "그래서 **활성화값들이 치우치면 표현력을 제한**한다는 관점에서 문제가 된다.\n",
    "\n",
    "각 층의 활성화 값은 적당히 고루 분포해야한다. 층과 층 사이에 적당하게 다양한 데이터가 흐르게 해야 신경망 학습이 효율적으로 이워지기 때문이다.\n",
    "반대로 치우친 데이터가 흐르면 기울기 손실이나 표현력에 제한 문제에 빠져서 학습이 잘 이뤄지지 않는 경우가 생긴다.\n",
    "\n",
    "Xavier 초깃값은 일반적인 딥러닝 프레임워크들이 표준적으로 이용하고 있다.\n",
    "앞계층의 노드가 n개라면 표준편차가 $\\frac{1}{\\sqrt{n}}$인 분포를 사용하면 된다는 결론을 이끌었다.\n",
    "\n",
    "![](/Users/taewoong/Documents/NLP_1/img/IMG_ABAB88F01496-1.jpeg)\n",
    "\n",
    "\n",
    "Xavier 초깃값은 활성화 함수가 신형인것을 전제로 이끈 결과이다. sigmoid 함수와 tanh 함수는 좌우대칭이라 중아 부근이 선형인 함수로 볼 수 있다.\n",
    "그래서 Xavier 초깃값이 적당하다. 반면 ReLu를 이용할 때는 He 초깃값을 사용한다. He 초깃값은 표준편차가 $\\sqrt\\frac{2}{n}$인 정규분포를 사용한다./\n",
    "ReLU는 음의 영역이 0이라서 더 넓게 분포시키기 위해 2배의 개수가 핋요하다고 해석할 수 있다.\n",
    "\n",
    "\n",
    "### 배치정규화 알고리즘\n",
    "\n",
    "* 학습을 빨리 진행할 수 있다.\n",
    "* 초깃값에 크게 의존하지 않는다.\n",
    "* 오버피팅을 억제한다.\n",
    "\n",
    "데이터 분포를 정규화하는 '배치 정규화 계층'을 신경망에 삽입한다.\n",
    "\n",
    "![](/Users/taewoong/Documents/NLP_1/img/IMG_EA01D612953C-1.jpeg)\n",
    "\n",
    "배치정규화는 그 이름과 같이 학습시 미니배치를 단위로 정규화한다. 구체적으로 데이터 분포가 평균이 0, 분산이 1이 되도록 정규화한다.\n",
    "\n",
    "$u_\\beta \\leftarrow \\frac{1}{m} \\sum_{i=1}^m x_i$\n",
    "\n",
    "$\\sigma^2_B \\leftarrow \\frac{1}{m} \\sum_{i=1}^m (x_i - u_\\beta)^2$\n",
    "\n",
    "$\\hat{x}_i \\leftarrow \\frac{x_i - u_\\beta}{\\sqrt{\\sigma_\\beta^2 + \\epsilon}} $\n",
    "\n",
    "미니배치 $B = \\{x_1, x_2, \\cdots, x_m\\}$ 이라는 m개의 입력데이터의 집합에 대해 평균 $u_B$와 분산 $\\sigma_B^2$을 구한다. 그리고 입력데이터를 평군이 0, 분산이 1이되게 정규화한다.\n",
    "$\\epsilon$ 기호는 작은 값으로 0으로 나누는 사태를 예방하기 위한 것이다.\n",
    "\n",
    "활성화 함수의 앞에 삽입함으러써 데이터 분포가 덜 치우치게 할 수 있다.\n",
    "또, 배치 정규화마다 정규화된 데이터에 확대와 이동 변환을 수행한다.\n",
    "\n",
    "$y_i \\leftarrow \\gamma \\hat{x}_i + B $\n",
    "\n",
    "이 식에서 $\\gamma$가 확대를, $B$가 이동을 담당한다. 두 값은 처음에는 $\\gamma = 1, B = 0 $부터 시작하고 학습하면서 적합한 값으로 조정해간다.\n",
    "\n",
    "![](https://mblogthumb-phinf.pstatic.net/MjAyMDAxMjlfMTg2/MDAxNTgwMjg0NDExOTUy._31NTMY36d0FM8MK-MfOT1WwDNjapvpkaCB_eW1mvyIg.FZqBmXpo3FKq-uarG7bL49QKw-FwiyXJhVG79TI34zIg.PNG.kkang9901/image.png?type=w800)\n",
    "\n",
    "\n",
    "### 오버피팅\n",
    "\n",
    "오버피팅은 주로 다음의 두 경우에 일어난다.\n",
    "* 매개변수가 많고 표현력이 높은 모델\n",
    "* 훈련 데이터가 적을 경우\n",
    "\n",
    "### 가중치 감소\n",
    "\n",
    "오버피팅 억제용으로 예로부터 많이 사용해온 가중치 감소 방법이다. 학습과정에서 큰 가중치에 대해서 그에 상응하는 큰 패널티를 부과하여 오버피팅을 억제하는 방법이다.\n",
    "\n",
    "### 드롭아웃\n",
    "\n",
    "신경망 모델이 복잡해지면 가중치 감소만으로 대응하기 어려워진다 이럴 때는 흔히 드롭아웃이라는 기법을 이용한다. 드롭아웃은 뉴런을 임의로 삭제하면서 학습하는 방법이다.\n",
    "훈련 때는 데이터를 흘릴떄 마나 삭제한 뉴런을 무작위로 선택하고, 시험 때는 모든 뉴런에 신호를 전달한다.\n",
    "\n",
    "![](https://lh3.googleusercontent.com/proxy/Q1-3Sal7bsPstfN8B7LFKJq1dgSqUicMQRM9assQCPAQNPXPD6qsOZ4lVaXFvu0IgPASowBL0MmSL1EZB8K35_zAN0sT_dVsHPXkr0TjC2EJAMgyNA)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}